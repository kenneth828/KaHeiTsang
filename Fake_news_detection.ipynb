{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmpuuh4YhTZ8"
      },
      "outputs": [],
      "source": [
        "from urllib.parse import urlparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import spacy\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_auc_score\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "data_real_path = '/content/politifact_real.csv'\n",
        "data_fake_path = '/content/politifact_fake.csv'\n",
        "\n",
        "try:\n",
        "    real_news = pd.read_csv(data_real_path)\n",
        "    fake_news = pd.read_csv(data_fake_path)\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: CSV files not found!\")\n",
        "    exit()\n",
        "\n",
        "real_news['label'] = 1\n",
        "fake_news['label'] = 0\n",
        "\n",
        "dataset = pd.concat([real_news, fake_news], ignore_index=True)\n",
        "dataset = dataset.dropna(subset=['title'])\n",
        "\n",
        "def extract_domain(url):\n",
        "    try:\n",
        "        domain = urlparse(url).netloc\n",
        "        return domain if domain else 'unknown_source'\n",
        "    except:\n",
        "        return 'unknown_source'\n",
        "\n",
        "dataset['source_domain'] = dataset['news_url'].apply(extract_domain)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "dataset['cleaned_title'] = dataset['title'].apply(clean_text)\n",
        "\n",
        "spacy.prefer_gpu()\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def extract_named_entities(text):\n",
        "    doc = nlp(text)\n",
        "    return len(doc.ents)\n",
        "\n",
        "dataset['ner_count'] = dataset['cleaned_title'].apply(extract_named_entities)\n",
        "\n",
        "political_jargon = {\"election\", \"senate\", \"congress\", \"campaign\", \"policy\", \"vote\", \"legislation\", \"republican\", \"democrat\",\n",
        "    \"veto\", \"incumbent\", \"mandate\", \"bill\", \"quorum\", \"gridlock\", \"jingoism\", \"kleptocracy\", \"lobby\", \"caucus\",\n",
        "    \"ballot\", \"nominee\", \"primary\", \"referendum\", \"coalition\", \"slush\", \"stump\", \"purge\", \"pivot\", \"surrogate\",\n",
        "    \"rider\", \"markup\", \"cloture\", \"hopper\", \"proxy\", \"spin\", \"dogma\", \"frontrunner\", \"turnout\", \"platform\",\n",
        "    \"populism\", \"cronyism\"}\n",
        "\n",
        "def identify_political_jargon(text):\n",
        "    words = set(text.split())\n",
        "    return len(words.intersection(political_jargon))\n",
        "\n",
        "dataset['jargon_count'] = dataset['cleaned_title'].apply(identify_political_jargon)\n",
        "\n",
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, encodings, labels, ner_features, jargon_features):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "        self.ner_features = ner_features\n",
        "        self.jargon_features = jargon_features\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        item['ner'] = torch.tensor(self.ner_features[idx], dtype=torch.float)\n",
        "        item['jargon'] = torch.tensor(self.jargon_features[idx], dtype=torch.float)\n",
        "        return item\n",
        "\n",
        "class CustomRobertaClassifier(nn.Module):\n",
        "    def __init__(self, model_name='roberta-base', num_labels=2, ner_dim=1, jargon_dim=1):\n",
        "        super().__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained(model_name)\n",
        "        self.fc = nn.Linear(768 + ner_dim + jargon_dim, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, ner, jargon, labels=None):\n",
        "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs[1]\n",
        "        combined = torch.cat((pooled_output, ner.unsqueeze(1), jargon.unsqueeze(1)), dim=1)\n",
        "        logits = self.fc(combined)\n",
        "        if labels is not None:\n",
        "            loss_fn = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "            loss = loss_fn(logits, labels)\n",
        "            return {'loss': loss, 'logits': logits}\n",
        "        return {'logits': logits}\n",
        "\n",
        "batch_sizes = [16, 32]\n",
        "learning_rates = [1e-5, 2e-5]\n",
        "best_accuracy = 0\n",
        "best_params = {}\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "X = dataset['cleaned_title']\n",
        "y = dataset['label']\n",
        "ner_features = dataset['ner_count']\n",
        "jargon_features = dataset['jargon_count']\n",
        "\n",
        "for bs in batch_sizes:\n",
        "    for lr in learning_rates:\n",
        "        fold_accuracies = []\n",
        "        print(f\"Batch Size: {bs}, Learning Rate: {lr}\")\n",
        "\n",
        "        for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
        "            print(f\"Fold {fold + 1}\")\n",
        "\n",
        "            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "            ner_train, ner_test = ner_features.iloc[train_idx], ner_features.iloc[test_idx]\n",
        "            jargon_train, jargon_test = jargon_features.iloc[train_idx], jargon_features.iloc[test_idx]\n",
        "\n",
        "            X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
        "            ner_train, ner_val = ner_train.iloc[:-len(X_val)], ner_train.iloc[-len(X_val):]\n",
        "            jargon_train, jargon_val = jargon_train.iloc[:-len(X_val)], jargon_train.iloc[-len(X_val):]\n",
        "\n",
        "            tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "            X_train_tokens = tokenizer(X_train.tolist(), max_length=128, padding='max_length', truncation=True, return_tensors='pt')\n",
        "            X_val_tokens = tokenizer(X_val.tolist(), max_length=128, padding='max_length', truncation=True, return_tensors='pt')\n",
        "            X_test_tokens = tokenizer(X_test.tolist(), max_length=128, padding='max_length', truncation=True, return_tensors='pt')\n",
        "\n",
        "            train_dataset = NewsDataset(X_train_tokens, y_train.tolist(), ner_train.tolist(), jargon_train.tolist())\n",
        "            val_dataset = NewsDataset(X_val_tokens, y_val.tolist(), ner_val.tolist(), jargon_val.tolist())\n",
        "            test_dataset = NewsDataset(X_test_tokens, y_test.tolist(), ner_test.tolist(), jargon_test.tolist())\n",
        "\n",
        "            train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
        "            val_loader = DataLoader(val_dataset, batch_size=bs)\n",
        "            test_loader = DataLoader(test_dataset, batch_size=bs)\n",
        "\n",
        "            model = CustomRobertaClassifier()\n",
        "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            model.to(device)\n",
        "\n",
        "            class_weights = torch.tensor(compute_class_weight('balanced', classes=np.array([0, 1]), y=y_train), dtype=torch.float)\n",
        "            optimizer = AdamW(model.parameters(), lr=lr)\n",
        "            scheduler = ReduceLROnPlateau(optimizer, 'min', patience=1, factor=0.5)\n",
        "\n",
        "            epochs = 10\n",
        "            best_val_loss = float('inf')\n",
        "            patience = 2\n",
        "            patience_counter = 0\n",
        "\n",
        "            for epoch in range(epochs):\n",
        "                model.train()\n",
        "                total_train_loss = 0\n",
        "                for batch in train_loader:\n",
        "                    optimizer.zero_grad()\n",
        "                    input_ids = batch['input_ids'].to(device)\n",
        "                    attention_mask = batch['attention_mask'].to(device)\n",
        "                    ner = batch['ner'].to(device)\n",
        "                    jargon = batch['jargon'].to(device)\n",
        "                    labels = batch['labels'].to(device)\n",
        "\n",
        "                    outputs = model(input_ids, attention_mask, ner, jargon, labels)\n",
        "                    loss = outputs['loss']\n",
        "                    total_train_loss += loss.item()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                model.eval()\n",
        "                total_val_loss = 0\n",
        "                with torch.no_grad():\n",
        "                    for batch in val_loader:\n",
        "                        input_ids = batch['input_ids'].to(device)\n",
        "                        attention_mask = batch['attention_mask'].to(device)\n",
        "                        ner = batch['ner'].to(device)\n",
        "                        jargon = batch['jargon'].to(device)\n",
        "                        labels = batch['labels'].to(device)\n",
        "                        outputs = model(input_ids, attention_mask, ner, jargon, labels)\n",
        "                        total_val_loss += outputs['loss'].item()\n",
        "\n",
        "                avg_train_loss = total_train_loss / len(train_loader)\n",
        "                avg_val_loss = total_val_loss / len(val_loader)\n",
        "                print(f\"Epoch {epoch + 1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "                scheduler.step(avg_val_loss)\n",
        "\n",
        "                if avg_val_loss < best_val_loss:\n",
        "                    best_val_loss = avg_val_loss\n",
        "                    patience_counter = 0\n",
        "                else:\n",
        "                    patience_counter += 1\n",
        "                    if patience_counter >= patience:\n",
        "                        print(\"Stopping early\")\n",
        "                        break\n",
        "\n",
        "            model.eval()\n",
        "            predictions = []\n",
        "            true_labels = []\n",
        "            with torch.no_grad():\n",
        "                for batch in test_loader:\n",
        "                    input_ids = batch['input_ids'].to(device)\n",
        "                    attention_mask = batch['attention_mask'].to(device)\n",
        "                    ner = batch['ner'].to(device)\n",
        "                    jargon = batch['jargon'].to(device)\n",
        "                    labels = batch['labels'].to(device)\n",
        "                    outputs = model(input_ids, attention_mask, ner, jargon)\n",
        "                    preds = torch.argmax(outputs['logits'], dim=1)\n",
        "                    predictions.extend(preds.cpu().tolist())\n",
        "                    true_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "            accuracy = accuracy_score(true_labels, predictions)\n",
        "            print(f\"Fold Accuracy: {accuracy * 100:.2f}%\")\n",
        "            print(classification_report(true_labels, predictions))\n",
        "            print(confusion_matrix(true_labels, predictions))\n",
        "            roc_auc = roc_auc_score(true_labels, predictions)\n",
        "            print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
        "            fold_accuracies.append(accuracy)\n",
        "\n",
        "        avg_accuracy = np.mean(fold_accuracies)\n",
        "        print(f\"Avg Accuracy for BS {bs}, LR {lr}: {avg_accuracy * 100:.2f}%\")\n",
        "        if avg_accuracy > best_accuracy:\n",
        "            best_accuracy = avg_accuracy\n",
        "            best_params = {'batch_size': bs, 'learning_rate': lr}\n",
        "\n",
        "print(f\"Best Params: {best_params}, Best Accuracy: {best_accuracy * 100:.2f}%\")"
      ]
    }
  ]
}